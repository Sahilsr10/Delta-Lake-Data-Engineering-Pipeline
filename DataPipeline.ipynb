{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce18e931-7e76-4a8a-9de5-8c0d1e9d3689",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faker in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (37.4.2)\nRequirement already satisfied: tzdata in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from faker) (2025.2)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# Install the faker library\n",
    "%pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bf78da7-854b-4e06-b93e-64d3c367b29a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Creating initial table with 5 rows ---\nInitial Delta table created.\n+----------------------------------------------------------+------------------------+--------------+\n|Address                                                   |Email                   |Name          |\n+----------------------------------------------------------+------------------------+--------------+\n|354 Ramirez Shoals Suite 082, Christopherborough, SC 30084|bcooley@example.net     |Darren Serrano|\n|22926 Marie Plains, East Scottburgh, NV 12714             |larsonjanice@example.net|Pamela Johnson|\n|063 Sarah Union Apt. 089, Port George, CO 92465           |joshua97@example.com    |Sandy Ortega  |\n|236 Brian Wells Apt. 554, Jennifermouth, GA 82695         |michael24@example.org   |Dennis Dalton |\n|84342 Williams Throughway Suite 673, Millerstad, DE 35766 |kcolon@example.net      |Crystal Baker |\n+----------------------------------------------------------+------------------------+--------------+\n\n\n--- Appending 3 new rows ---\nAppend operation complete.\nTotal rows after append: 8\n+----------------------------------------------------------+------------------------+--------------+\n|Address                                                   |Email                   |Name          |\n+----------------------------------------------------------+------------------------+--------------+\n|354 Ramirez Shoals Suite 082, Christopherborough, SC 30084|bcooley@example.net     |Darren Serrano|\n|22926 Marie Plains, East Scottburgh, NV 12714             |larsonjanice@example.net|Pamela Johnson|\n|063 Sarah Union Apt. 089, Port George, CO 92465           |joshua97@example.com    |Sandy Ortega  |\n|236 Brian Wells Apt. 554, Jennifermouth, GA 82695         |michael24@example.org   |Dennis Dalton |\n|84342 Williams Throughway Suite 673, Millerstad, DE 35766 |kcolon@example.net      |Crystal Baker |\n|USS Hawkins, FPO AA 33807                                 |ngiles@example.org      |Shawn Martin  |\n|1713 Joshua Spurs, Robertmouth, PA 24740                  |amartin@example.com     |Linda Flowers |\n|Unit 7879 Box 3141, DPO AA 91812                          |brittany87@example.com  |Jay Parrish   |\n+----------------------------------------------------------+------------------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from delta.tables import DeltaTable\n",
    "from faker import Faker\n",
    "import pandas as pd\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the path for your Delta table in the Databricks File System (DBFS)\n",
    "delta_table_path = \"/tmp/delta/user_data\"\n",
    "\n",
    "# --- 1. Initialize Spark Session with Time Zone ---\n",
    "# Setting the time zone is crucial for timestamp operations later\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaDataPipeline\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"Asia/Kolkata\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# --- 2. Function to Generate Fake Data ---\n",
    "fake = Faker()\n",
    "def generate_fake_data(num_rows):\n",
    "    \"\"\"Generates a list of fake user data.\"\"\"\n",
    "    data = [{\n",
    "        \"Name\": fake.name(),\n",
    "        \"Address\": fake.address().replace(\"\\n\", \", \"),\n",
    "        \"Email\": fake.email()\n",
    "    } for _ in range(num_rows)]\n",
    "    return data\n",
    "\n",
    "# --- 3. Create and Write Initial Data (First Run) ---\n",
    "print(\"--- Creating initial table with 5 rows ---\")\n",
    "initial_data = generate_fake_data(5)\n",
    "initial_df = spark.createDataFrame(initial_data)\n",
    "\n",
    "# Write data to Delta Lake format, overwriting if it already exists for a clean start\n",
    "initial_df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
    "\n",
    "print(\"Initial Delta table created.\")\n",
    "# Read and display the full table\n",
    "initial_table = spark.read.format(\"delta\").load(delta_table_path)\n",
    "initial_table.show(truncate=False)\n",
    "\n",
    "# --- 4. Append New Data using DeltaTable API ---\n",
    "print(\"\\n--- Appending 3 new rows ---\")\n",
    "new_data = generate_fake_data(3)\n",
    "new_df = spark.createDataFrame(new_data)\n",
    "\n",
    "# Use the DeltaTable API to append\n",
    "delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
    "new_df.write.format('delta').mode('append').save(delta_table_path)\n",
    "\n",
    "print(\"Append operation complete.\")\n",
    "# Retrieve and display the latest version of the table\n",
    "latest_table = spark.read.format(\"delta\").load(delta_table_path)\n",
    "print(f\"Total rows after append: {latest_table.count()}\")\n",
    "latest_table.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90199c88-99c8-422b-858e-4d8e8655242b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Appending 4 more rows incrementally ---\nIncremental append complete.\nTotal rows now: 12\n+------------------------------------------------------------+---------------------------+-------------------+\n|Address                                                     |Email                      |Name               |\n+------------------------------------------------------------+---------------------------+-------------------+\n|354 Ramirez Shoals Suite 082, Christopherborough, SC 30084  |bcooley@example.net        |Darren Serrano     |\n|22926 Marie Plains, East Scottburgh, NV 12714               |larsonjanice@example.net   |Pamela Johnson     |\n|063 Sarah Union Apt. 089, Port George, CO 92465             |joshua97@example.com       |Sandy Ortega       |\n|236 Brian Wells Apt. 554, Jennifermouth, GA 82695           |michael24@example.org      |Dennis Dalton      |\n|84342 Williams Throughway Suite 673, Millerstad, DE 35766   |kcolon@example.net         |Crystal Baker      |\n|95390 Mosley Estates Suite 412, North Mckenziestad, MN 86710|gutierrezsydney@example.net|Brenda Wright      |\n|2047 Robert Manors Suite 876, Alexanderville, CT 15389      |karenortiz@example.com     |Rachael Hopkins    |\n|229 Stephen Gateway Suite 673, East Jamestown, NV 64413     |debbie87@example.com       |John Miller        |\n|888 Cline Station Suite 520, Andrewton, CT 27161            |serranojessica@example.com |Christopher Ballard|\n|USS Hawkins, FPO AA 33807                                   |ngiles@example.org         |Shawn Martin       |\n|1713 Joshua Spurs, Robertmouth, PA 24740                    |amartin@example.com        |Linda Flowers      |\n|Unit 7879 Box 3141, DPO AA 91812                            |brittany87@example.com     |Jay Parrish        |\n+------------------------------------------------------------+---------------------------+-------------------+\n\n\n--- Displaying Delta Table History ---\n+-------+-------------------+---------+------------------------------------------------------------+\n|version|timestamp          |operation|operationParameters                                         |\n+-------+-------------------+---------+------------------------------------------------------------+\n|3      |2025-07-22 07:11:29|WRITE    |{mode -> Append, statsOnLoad -> false, partitionBy -> []}   |\n|2      |2025-07-22 07:10:28|WRITE    |{mode -> Append, statsOnLoad -> false, partitionBy -> []}   |\n|1      |2025-07-22 07:10:25|WRITE    |{mode -> Overwrite, statsOnLoad -> false, partitionBy -> []}|\n|0      |2025-07-22 07:09:57|WRITE    |{mode -> Overwrite, statsOnLoad -> false, partitionBy -> []}|\n+-------+-------------------+---------+------------------------------------------------------------+\n\n\n--- Retrieving data from Version 0 ---\n+------------------------------------------------+----------------------------+----------------+\n|Address                                         |Email                       |Name            |\n+------------------------------------------------+----------------------------+----------------+\n|6011 Peter Rue, Josephmouth, DC 99248           |nlong@example.net           |Michelle Francis|\n|1593 Marshall Forest, Cynthiaside, WY 04876     |williamssamantha@example.com|Jennifer Cook   |\n|278 Leslie Lake, North Jennifer, AS 32751       |wlittle@example.org         |Edward Woods    |\n|6085 Chen Forge Suite 978, Annachester, MS 39205|phyllis58@example.org       |Barry Good      |\n|1437 Snyder Lake Suite 528, Garyhaven, AK 71381 |clintonmiller@example.net   |Ashley Moreno   |\n+------------------------------------------------+----------------------------+----------------+\n\n\n--- Retrieving data from timestamp '2025-07-22 07:10:25' (Version 1) ---\n+----------------------------------------------------------+------------------------+--------------+\n|Address                                                   |Email                   |Name          |\n+----------------------------------------------------------+------------------------+--------------+\n|354 Ramirez Shoals Suite 082, Christopherborough, SC 30084|bcooley@example.net     |Darren Serrano|\n|22926 Marie Plains, East Scottburgh, NV 12714             |larsonjanice@example.net|Pamela Johnson|\n|063 Sarah Union Apt. 089, Port George, CO 92465           |joshua97@example.com    |Sandy Ortega  |\n|236 Brian Wells Apt. 554, Jennifermouth, GA 82695         |michael24@example.org   |Dennis Dalton |\n|84342 Williams Throughway Suite 673, Millerstad, DE 35766 |kcolon@example.net      |Crystal Baker |\n+----------------------------------------------------------+------------------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import col\n",
    "import time\n",
    "\n",
    "# --- Configuration for this run ---\n",
    "NUM_NEW_ROWS_TO_ADD = 4\n",
    "\n",
    "# --- 1. Append a specified number of new rows ---\n",
    "print(f\"--- Appending {NUM_NEW_ROWS_TO_ADD} more rows incrementally ---\")\n",
    "incremental_data = generate_fake_data(NUM_NEW_ROWS_TO_ADD)\n",
    "incremental_df = spark.createDataFrame(incremental_data)\n",
    "\n",
    "delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
    "incremental_df.write.format('delta').mode('append').save(delta_table_path)\n",
    "\n",
    "print(\"Incremental append complete.\")\n",
    "latest_table = spark.read.format(\"delta\").load(delta_table_path)\n",
    "print(f\"Total rows now: {latest_table.count()}\")\n",
    "latest_table.show(truncate=False)\n",
    "\n",
    "# --- 2. Track and Display Table Versions (History) ---\n",
    "print(\"\\n--- Displaying Delta Table History ---\")\n",
    "delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
    "history_df = delta_table.history()\n",
    "\n",
    "# Display the full history, showing each operation (WRITE, MERGE, etc.)\n",
    "history_df.select(\"version\", \"timestamp\", \"operation\", \"operationParameters\").show(truncate=False)\n",
    "\n",
    "# --- 3. Retrieve Data from a Previous Version ---\n",
    "# Let's retrieve the first version (v0) of the table\n",
    "print(\"\\n--- Retrieving data from Version 0 ---\")\n",
    "try:\n",
    "    version_zero_df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_table_path)\n",
    "    version_zero_df.show(truncate=False)\n",
    "except Exception as e:\n",
    "    print(f\"Could not retrieve version 0. Error: {e}\")\n",
    "\n",
    "# --- 4. Retrieve Data using a Timestamp ---\n",
    "# Get the timestamp of version 1 to query it\n",
    "try:\n",
    "    timestamp_v1 = history_df.filter(col(\"version\") == 1).select(\"timestamp\").first()[0]\n",
    "    timestamp_str = timestamp_v1.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    print(f\"\\n--- Retrieving data from timestamp '{timestamp_str}' (Version 1) ---\")\n",
    "    timestamp_df = spark.read.format(\"delta\").option(\"timestampAsOf\", timestamp_str).load(delta_table_path)\n",
    "    timestamp_df.show(truncate=False)\n",
    "except Exception as e:\n",
    "    print(f\"Could not retrieve by timestamp. Maybe version 1 doesn't exist? Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af591406-018d-4cd9-b818-46fc06714788",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline run. Appending 5 new rows.\nSuccessfully appended 5 rows.\nEmail notification sent successfully.\nPipeline execution finished.\n"
     ]
    }
   ],
   "source": [
    "# Import all necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from delta.tables import DeltaTable\n",
    "from faker import Faker\n",
    "import pandas as pd\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "delta_table_path = \"/tmp/delta/user_data\"\n",
    "NUM_ROWS_TO_APPEND = 5  # Number of new rows to add in each run\n",
    "SENDER_EMAIL = \"sahilsrivastava773@gmail.com\"\n",
    "RECIPIENT_EMAIL = \"sahilsriv773@gmail.com\"\n",
    "\n",
    "# --- 2. Initialize Spark Session ---\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ScheduledDeltaPipeline\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"Asia/Kolkata\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# --- 3. Define Helper Functions ---\n",
    "fake = Faker()\n",
    "def generate_fake_data(num_rows):\n",
    "    \"\"\"Generates a list of fake user data.\"\"\"\n",
    "    return [{\n",
    "        \"Name\": fake.name(),\n",
    "        \"Address\": fake.address().replace(\"\\n\", \", \"),\n",
    "        \"Email\": fake.email()\n",
    "    } for _ in range(num_rows)]\n",
    "\n",
    "def send_summary_email(summary_html, rows_appended):\n",
    "    \"\"\"Sends an email notification with a nicely designed HTML summary.\"\"\"\n",
    "    try:\n",
    "        # Retrieve the stored app password from Databricks secrets\n",
    "        app_password = dbutils.secrets.get(scope=\"email_creds\", key=\"app-password\")\n",
    "        \n",
    "        # Email content\n",
    "        subject = f\"Data Pipeline Success: {rows_appended} New Rows Appended\"\n",
    "        message = MIMEMultipart(\"alternative\")\n",
    "        message[\"Subject\"] = subject\n",
    "        message[\"From\"] = SENDER_EMAIL\n",
    "        message[\"To\"] = RECIPIENT_EMAIL\n",
    "        \n",
    "        # --- New HTML Template with CSS ---\n",
    "        body = f\"\"\"\n",
    "        <html>\n",
    "        <head>\n",
    "        <style>\n",
    "            body {{\n",
    "                font-family: Arial, sans-serif;\n",
    "                background-color: #f4f4f9;\n",
    "                margin: 0;\n",
    "                padding: 0;\n",
    "            }}\n",
    "            .container {{\n",
    "                max-width: 700px;\n",
    "                margin: 20px auto;\n",
    "                background-color: #ffffff;\n",
    "                border-radius: 8px;\n",
    "                box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n",
    "                overflow: hidden;\n",
    "            }}\n",
    "            .header {{\n",
    "                background-color: #4a90e2;\n",
    "                color: #ffffff;\n",
    "                padding: 20px;\n",
    "                text-align: center;\n",
    "            }}\n",
    "            .header h2 {{\n",
    "                margin: 0;\n",
    "            }}\n",
    "            .content {{\n",
    "                padding: 30px;\n",
    "                line-height: 1.6;\n",
    "                color: #333333;\n",
    "            }}\n",
    "            .content p {{\n",
    "                margin: 0 0 15px 0;\n",
    "            }}\n",
    "            .data-table {{\n",
    "                width: 100%;\n",
    "                border-collapse: collapse;\n",
    "                margin-top: 20px;\n",
    "            }}\n",
    "            .data-table th, .data-table td {{\n",
    "                border: 1px solid #dddddd;\n",
    "                padding: 12px;\n",
    "                text-align: left;\n",
    "            }}\n",
    "            .data-table th {{\n",
    "                background-color: #f2f2f2;\n",
    "                font-weight: bold;\n",
    "            }}\n",
    "            .footer {{\n",
    "                background-color: #f4f4f9;\n",
    "                color: #888888;\n",
    "                font-size: 12px;\n",
    "                text-align: center;\n",
    "                padding: 20px;\n",
    "            }}\n",
    "        </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            <div class=\"container\">\n",
    "                <div class=\"header\">\n",
    "                    <h2>Data Ingestion Summary</h2>\n",
    "                </div>\n",
    "                <div class=\"content\">\n",
    "                    <p>The pipeline ran successfully and appended <strong>{rows_appended}</strong> new rows to the Delta table.</p>\n",
    "                    <h3>Appended Data:</h3>\n",
    "                    {summary_html}\n",
    "                </div>\n",
    "                <div class=\"footer\">\n",
    "                    <p>This is an automated notification from your Azure Databricks pipeline.</p>\n",
    "                </div>\n",
    "            </div>\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "        message.attach(MIMEText(body, \"html\"))\n",
    "\n",
    "        # Connect to Gmail's SMTP server and send the email\n",
    "        with smtplib.SMTP(\"smtp.gmail.com\", 587) as server:\n",
    "            server.starttls()\n",
    "            server.login(SENDER_EMAIL, app_password)\n",
    "            server.sendmail(SENDER_EMAIL, RECIPIENT_EMAIL, message.as_string())\n",
    "        print(\"Email notification sent successfully.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error sending email: {e}\")\n",
    "\n",
    "# --- 4. Main Pipeline Logic ---\n",
    "print(f\"Starting pipeline run. Appending {NUM_ROWS_TO_APPEND} new rows.\")\n",
    "new_data = generate_fake_data(NUM_ROWS_TO_APPEND)\n",
    "\n",
    "if not new_data:\n",
    "    print(\"No new data to append. Exiting.\")\n",
    "else:\n",
    "    new_df = spark.createDataFrame(pd.DataFrame(new_data))\n",
    "    \n",
    "    # Append the new data\n",
    "    new_df.write.format(\"delta\").mode(\"append\").save(delta_table_path)\n",
    "    \n",
    "    print(f\"Successfully appended {len(new_data)} rows.\")\n",
    "    \n",
    "    # --- 5. Prepare and Send Notification ---\n",
    "    # Convert the newly appended data to an HTML table for the email\n",
    "    summary_html = pd.DataFrame(new_data).to_html(index=False, classes='data-table')\n",
    "    \n",
    "    # Send the email\n",
    "    send_summary_email(summary_html, len(new_data))\n",
    "\n",
    "print(\"Pipeline execution finished.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DataPipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}